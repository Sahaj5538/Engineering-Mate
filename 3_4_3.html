<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Model Evaluation in Machine Learning</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f4f8;
            margin: 20px;
            line-height: 1.7;
        }
        h1, h2 {
            color: #2c3e50;
        }
        section {
            background-color: #ffffff;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        }
        ul {
            margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>Model Evaluation in Machine Learning</h1>

<section>
    <h2>Introduction</h2>
    <p><strong>Model Evaluation</strong> is the process of assessing how well a machine learning model performs on unseen data. It ensures that the model generalizes well beyond the training data and helps compare different models objectively.</p>
</section>

<section>
    <h2>Why Model Evaluation is Important</h2>
    <ul>
        <li>To avoid overfitting or underfitting.</li>
        <li>To select the best model for deployment.</li>
        <li>To ensure the model is reliable and robust.</li>
        <li>To quantify model performance using metrics.</li>
    </ul>
</section>

<section>
    <h2>Key Concepts</h2>
    <ul>
        <li><strong>Training Set:</strong> The data used to train the model.</li>
        <li><strong>Validation Set:</strong> Data used to tune the model's hyperparameters.</li>
        <li><strong>Test Set:</strong> Data used to evaluate the final model's performance.</li>
        <li><strong>Overfitting:</strong> When a model performs well on training data but poorly on new data.</li>
        <li><strong>Underfitting:</strong> When a model performs poorly even on the training data.</li>
    </ul>
</section>

<section>
    <h2>Evaluation Metrics</h2>

    <h3>For Classification Models:</h3>
    <ul>
        <li><strong>Accuracy:</strong> Ratio of correctly predicted instances to total instances.</li>
        <li><strong>Precision:</strong> Correct positive predictions divided by total positive predictions.</li>
        <li><strong>Recall (Sensitivity):</strong> Correct positive predictions divided by actual positives.</li>
        <li><strong>F1 Score:</strong> Harmonic mean of Precision and Recall.</li>
        <li><strong>Confusion Matrix:</strong> Table showing correct and incorrect predictions.</li>
        <li><strong>ROC Curve and AUC:</strong> Graph showing the performance across all classification thresholds.</li>
    </ul>

    <h3>For Regression Models:</h3>
    <ul>
        <li><strong>Mean Absolute Error (MAE):</strong> Average of the absolute errors.</li>
        <li><strong>Mean Squared Error (MSE):</strong> Average of the squared errors.</li>
        <li><strong>Root Mean Squared Error (RMSE):</strong> Square root of the mean squared error.</li>
        <li><strong>RÂ² Score (Coefficient of Determination):</strong> Measures the proportion of the variance explained by the model.</li>
    </ul>
</section>

<section>
    <h2>Validation Techniques</h2>
    <ul>
        <li><strong>Hold-Out Validation:</strong> Split data into training and testing sets.</li>
        <li><strong>K-Fold Cross Validation:</strong> Data is divided into K subsets; each subset is used as a test set once.</li>
        <li><strong>Stratified K-Fold:</strong> Ensures each fold has the same proportion of class labels.</li>
        <li><strong>Leave-One-Out Cross Validation (LOOCV):</strong> Each instance is used once as a test set while the rest form the training set.</li>
    </ul>
</section>

<section>
    <h2>Bias-Variance Tradeoff</h2>
    <p>Understanding the bias-variance tradeoff is crucial for building good models:
    <ul>
        <li><strong>High Bias:</strong> Leads to underfitting.</li>
        <li><strong>High Variance:</strong> Leads to overfitting.</li>
        <li><strong>Goal:</strong> Find the right balance between bias and variance.</li>
    </ul>
    </p>
</section>

<section>
    <h2>Conclusion</h2>
    <p>Model evaluation is a critical step in the machine learning workflow. It ensures the chosen model performs well on new data, helps identify problems like overfitting, and guides us in improving model performance systematically.</p>
</section>

</body>
</html>
