<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sorting and Algorithm Analysis</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        h1, h2, h3 {
            color: #333;
        }
        ul {
            margin-left: 20px;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 5px;
            border-radius: 4px;
        }
        section {
            margin-bottom: 40px;
        }
    </style>
</head>
<body>

    <h1>UNIT I: Sorting and Algorithm Analysis</h1>

    <section>
        <h2>Sorting Algorithms</h2>

        <h3>Bubble Sort</h3>
        <p>Bubble Sort is a simple comparison-based algorithm. It repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. This process continues until no swaps are needed, indicating that the list is sorted.</p>

        <h3>Insertion Sort</h3>
        <p>Insertion Sort builds the sorted list one item at a time. It takes each element from the input and finds the correct position within the already-sorted elements, shifting larger elements to the right to make space.</p>

        <h3>Selection Sort</h3>
        <p>Selection Sort divides the list into two parts: the sorted part at the beginning and the unsorted part at the end. It repeatedly selects the smallest (or largest) element from the unsorted section and moves it to the end of the sorted section.</p>

        <h3>Quick Sort</h3>
        <p>Quick Sort is a highly efficient divide-and-conquer algorithm. It picks an element as a pivot and partitions the array around the pivot, recursively sorting the sub-arrays.</p>

        <h3>Merge Sort</h3>
        <p>Merge Sort is a stable, comparison-based divide-and-conquer algorithm. It divides the array into halves, recursively sorts them, and then merges the sorted halves back together.</p>

        <h3>Radix Sort</h3>
        <p>Radix Sort is a non-comparative sorting algorithm. It processes integer keys by grouping digits at each significant place and sorting based on each digit, typically using Counting Sort as a subroutine.</p>

        <h3>Counting Sort</h3>
        <p>Counting Sort is a non-comparative sorting technique based on keys being small integers. It counts the number of occurrences of each unique element and uses this information to place elements directly into the sorted position.</p>
    </section>

    <section>
        <h2>Algorithms and Analysis</h2>

        <h3>Algorithm Analysis: Introduction to Analyzing Algorithm Efficiency</h3>
        <p>Algorithm analysis involves predicting the resources an algorithm requires. Most commonly, we analyze:</p>
        <ul>
            <li>Time Complexity: How the runtime increases relative to input size.</li>
            <li>Space Complexity: How the memory usage increases relative to input size.</li>
        </ul>

        <h3>Basic Concepts</h3>
        <p><strong>Order of Complexity</strong> refers to how the resource requirements (time or space) grow as the size of the input increases.</p>

        <h3>Asymptotic Notations</h3>
        <p>Asymptotic notations describe the limiting behavior of an algorithm’s resource consumption, focusing on input size approaching infinity.</p>

        <h4>Big-O (O): Upper Bound</h4>
        <p>Big-O notation provides the worst-case scenario for an algorithm's growth rate. It defines an upper bound. Example: If an algorithm runs in <code>O(n^2)</code> time, its runtime will not grow faster than <code>n²</code> for large <code>n</code>.</p>

        <h4>Omega (Ω): Lower Bound</h4>
        <p>Omega notation provides the best-case scenario for an algorithm's growth rate. It defines a lower bound. Example: If an algorithm runs in <code>Ω(n)</code> time, it will take at least <code>n</code> operations in the best case.</p>

        <h4>Theta (Θ): Tight Bound</h4>
        <p>Theta notation describes the exact asymptotic behavior. If a function is in <code>Θ(f(n))</code>, then it is bounded both above and below by <code>f(n)</code> for large <code>n</code>.</p>

        <h4>Little-o (o): Upper Bound but Not Tight</h4>
        <p>Little-o notation describes an upper bound that is not tight. It means the function grows strictly slower than <code>f(n)</code>. Example: <code>o(n^2)</code> means the algorithm grows slower than <code>n²</code>, but not as fast as exactly <code>n²</code>.</p>

        <h4>Little-omega (ω): Lower Bound but Not Tight</h4>
        <p>Little-omega notation describes a lower bound that is not tight. It indicates that the algorithm's growth is strictly faster than <code>f(n)</code>, but not at the exact rate of <code>f(n)</code>.</p>
    </section>

</body>
</html>
